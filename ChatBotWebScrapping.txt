import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document


# Now you can pass 'langchain_docs' to your ChromaDB/FAISS vector store.
# --- CONFIGURATION ---
BASE_URL = "https://www.ecc.de/en/risk-management/"
DOMAIN_NAME = urlparse(BASE_URL).netloc

def is_internal(url):
    # ---Check if the link belongs to the same domain.----
    return urlparse(url).netloc == DOMAIN_NAME

def crawl_site(start_url):
    to_visit = {start_url}
    visited = set()
    all_text_content = {}
    all_pdfs = set()

    while to_visit:
        current_url = to_visit.pop()
        if current_url in visited:
            continue
            
        try:
            print(f"Scraping: {current_url}")
            response = requests.get(current_url, timeout=10)
            visited.add(current_url)
            
            if 'text/html' not in response.headers.get('Content-Type', ''):
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. Extract and store text for this specific page
            page_text = soup.get_text(separator=' ', strip=True)
            all_text_content[current_url] = page_text

            # 2. Find all links on the page
            for link in soup.find_all('a', href=True):
                full_url = urljoin(current_url, link['href']).split('#')[0].rstrip('/')
                
                # If it's a PDF, add to PDF list
                if full_url.lower().endswith('.pdf'):
                    all_pdfs.add(full_url)
                
                # If it's an internal webpage we haven't seen, add to queue
                elif is_internal(full_url) and full_url not in visited:
                    to_visit.add(full_url)

        except Exception as e:
            print(f"Failed to crawl {current_url}: {e}")
# 
    return all_text_content, list(all_pdfs)

# --- EXECUTION ---
# This will return a dictionary of {url: text} and a list of unique PDFs
site_data, pdf_list = crawl_site(BASE_URL)
print(f"\n Crawling Complete!")
print(f" Total Pages Scanned: {len(site_data)}")
print(f"Total PDFs Found: {len(pdf_list)}")


# Convert our dictionary into LangChain documents with metadata
langchain_docs = []
for url, text in site_data.items():
    new_doc = Document(page_content=text, metadata={"source": url})
    langchain_docs.append(new_doc)